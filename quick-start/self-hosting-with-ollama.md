---
description: Run popular LLMs like Llama, Gemma and dozens of others locally for free
icon: ram
---

# Self-Hosting with Ollama

## Install Ollama App <a href="#ollama-setup" id="ollama-setup"></a>

Install the **Ollama** application locally on your machine: press the **Download** button at [**ollama.com**](https://ollama.com/) to download the **Ollama** app. Make sure the app is running after installation.

<figure><img src="../.gitbook/assets/image (95).png" alt=""><figcaption></figcaption></figure>

## Install Ollama Model

Install at least one Ollama model, either manually from the [Ollama website](https://ollama.com/library), or using the Terminal command `ollama run <model_name>`. For example, to install the _llama 3.2_ model, use the command `ollama run llama3.2`.

## Test & Configure in Unity

{% hint style="info" %}
### Edit ▶ Preferences ▶︎ AI Dev Kit ▶︎ Ollama <a href="#ollama-setup" id="ollama-setup"></a>
{% endhint %}

<figure><img src="../.gitbook/assets/image (96).png" alt=""><figcaption></figcaption></figure>

To make sure that your server is running, test the connection using the **Test Connection** button.
