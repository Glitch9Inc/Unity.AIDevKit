---
icon: ram
---

# Ollama

Worried about costs? No Problem! With Ollama you can run LLMs like **LLaMA** and **Mistral** on your own machine for **free**. Ideal for offline development, edge applications, and private inference without relying on cloud services.

<table><thead><tr><th width="100" data-type="checkbox">Supported</th><th>All Ollama APIs (2025.04.23)</th><th data-hidden>All OpenAI APIs (2025.05.18)</th></tr></thead><tbody><tr><td>true</td><td><span data-gb-custom-inline data-tag="emoji" data-code="1f4ac">💬</span> Chat Completion, Stream, Completion</td><td><span data-gb-custom-inline data-tag="emoji" data-code="1f4ac">💬</span> Chat Completions, Completions (Legacy)</td></tr><tr><td>true</td><td>🧠 Embeddings</td><td></td></tr><tr><td>true</td><td>🛠️ Models, Model Management</td><td></td></tr><tr><td>false</td><td>🛠️ Version</td><td></td></tr></tbody></table>
